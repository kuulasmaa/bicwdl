include required(classpath("application"))

workflow-options {
    # By default Cromwell erases the per workflow logs when the workflow completes to reduce disk usage.
    workflow-log-temporary = true
    workflow-failure_mode = "NoNewCalls"
}

call-caching {
    enabled = false
    invalidate-bad-cache-results = true
    #write-to-cache = false
    #read-from-cache = false
}

backend {
    default = local

    providers {

        local {
            # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend.
            actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

            # The backend custom configuration.
            config {
                # Optional limits on the number of concurrent jobs
                #concurrent-job-limit = 5

                # If true submits scripts to the bash background using "&". Only usefull for dispatchers that do NOT submit
                # the job and then immediately return a scheduled job id.
                run-in-background = true

                # `temporary-directory` creates the temporary directory for commands.
                #
                # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:
                # temporary-directory = "$(mktemp -d \"$PWD\"/tmp.XXXXXX)"
                #
                # The expression is run from the execution directory for the script. The expression must create the directory
                # if it does not exist, and then return the full path to the directory.
                #
                # To create and return a non-random temporary directory, use something like:
                # temporary-directory = "$(mkdir -p /tmp/mydir && echo /tmp/mydir)"

                # `script-epilogue` configures a shell command to run after the execution of every command block.
                #
                # If this value is not set explicitly, the default value is `sync`, equivalent to:
                # script-epilogue = "sync"
                #
                # To turn off the default `sync` behavior set this value to an empty string:
                # script-epilogue = ""

                # `glob-link-command` specifies command used to link glob outputs, by default using hard-links.
                # If filesystem doesn't allow hard-links (e.g., beeGFS), change to soft-links as follows:
                # glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"

                # The list of possible runtime custom attributes.
                #job-shell="/bin/bash"
                run-in-background = true
                runtime-attributes = """
                String? docker
                String? docker_user
                """

                # Submit string when there is no "docker" runtime attribute.
                submit = "/usr/bin/env bash ${script}"
                #submit = "bash ${script}"
                #submit = "singularity exec ${sing_image} ${job_shell} ${script}"
                #submit = "singularity exec ${sing_image} ${job_shell} ${script}"

                # Submit string when there is a "docker" runtime attribute.
                #submit-docker = """
                #docker run --rm -i ${"--user " + docker_user} --entrypoint ${job_shell} -v ${cwd}:${docker_cwd} ${docker} ${script}
                #"""
                submit-docker = """
                singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}
                """

                # Root directory where Cromwell writes job results. This directory must be
                # visible and writeable by the Cromwell process as well as the jobs that Cromwell
                # launches.
                root = "cromwell-executions"

                # Root directory where Cromwell writes job results in the container. This value
                # can be used to specify where the execution folder is mounted in the container.
                # it is used for the construction of the docker_cwd string in the submit-docker
                # value above.
                dockerRoot = "/cromwell-executions"

                # File system configuration.
                filesystems {
                    # For SFS backends, the "local" configuration specifies how files are handled.
                    local {
                        # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
                        localization: [
                          "hard-link", "soft-link", "copy"
                        ]

                        # Call caching strategies
                        caching {
                          # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
                          duplication-strategy: [
                            "hard-link", "soft-link", "copy"
                          ]

                          # Possible values: file, path, path+modtime
                          # "file" will compute an md5 hash of the file content.
                          # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
                          # in order to allow for the original file path to be hashed.
                          # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
                          # Default: file
                          hashing-strategy: "file"

                          # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
                          # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
                          check-sibling-md5: false
                        }
                    }
                }
                # The defaults for runtime attributes if not provided.
                default-runtime-attributes {
                  failOnStderr: false
                  continueOnReturnCode: 0
                }
            }
        }

        slurm-docker {
          actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
          config {
            runtime-attributes = """
            String queue = "serial"
            Int runtime_minutes = 600
            Int cpus = 2
            Int requested_memory_mb_per_core = 8000
            String? docker
            """

            submit = """
                sbatch \
                  --wait \
                  -J ${job_name} \
                  -D ${cwd} \
                  -o ${out} \
                  -e ${err} \
                  -t ${runtime_minutes} \
                  ${"-c " + cpus} \
                  --mem-per-cpu=${requested_memory_mb_per_core} \
                  --wrap "/bin/bash ${script}"
            """

            submit-docker = """
                # Ensure singularity is loaded if it's installed as a module
                #module load Singularity/3.0.1

                # Build the Docker image into a singularity image
                DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
                IMAGE=${cwd}/$DOCKER_NAME.sif
                if [ ! -f $IMAGE ]; then
                    singularity pull $IMAGE docker://${docker}
                fi

                # Submit the script to SLURM
                sbatch \
                  --wait \
                  -J ${job_name} \
                  -D ${cwd} \
                  -o ${cwd}/execution/stdout \
                  -e ${cwd}/execution/stderr \
                  -t ${runtime_minutes} \
                  ${"-c " + cpus} \
                  --mem-per-cpu=${requested_memory_mb_per_core} \
                  --wrap "singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}"
            """

            kill = "scancel ${job_id}"
            check-alive = "squeue -j ${job_id}"
            job-id-regex = "Submitted batch job (\\d+).*"

            # File system configuration.
            filesystems {
                # For SFS backends, the "local" configuration specifies how files are handled.
                local {
                    # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
                    localization: [
                      "hard-link", "soft-link", "copy"
                    ]

                    # Call caching strategies
                    caching {
                      # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
                      duplication-strategy: [
                        "hard-link", "soft-link", "copy"
                      ]

                      # Possible values: file, path, path+modtime
                      # "file" will compute an md5 hash of the file content.
                      # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
                      # in order to allow for the original file path to be hashed.
                      # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
                      # Default: file
                      hashing-strategy: "file"

                      # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
                      # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
                      check-sibling-md5: false
                     }
                }
            }
          }
        }

        slurm {
          actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
          config {
            runtime-attributes = """
            Int runtime_minutes = 600
            Int cpus = 2
            Int requested_memory_mb_per_core = 8000
            String queue = "short"
            """

            # If an 'exit-code-timeout-seconds' value is specified:
            # - check-alive will be run at this interval for every job
            # - if a job is found to be not alive, and no RC file appears after this interval
            # - Then it will be marked as Failed.
            # Warning: If set, Cromwell will run 'check-alive' for every job at this interval

            # exit-code-timeout-seconds = 120

            submit = """
                sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \
                ${"-n " + cpus} \
                --mem-per-cpu=${requested_memory_mb_per_core} \
                --wrap "/usr/bin/env bash ${script}"
            """
            kill = "scancel ${job_id}"
            check-alive = "squeue -j ${job_id}"
            job-id-regex = "Submitted batch job (\\d+).*"

            # File system configuration.
            filesystems {
                # For SFS backends, the "local" configuration specifies how files are handled.
                local {
                    # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files.
                    localization: [
                      "hard-link", "soft-link", "copy"
                    ]

                    # Call caching strategies
                    caching {
                      # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
                      duplication-strategy: [
                        "hard-link", "soft-link", "copy"
                      ]

                      # Possible values: file, path, path+modtime
                      # "file" will compute an md5 hash of the file content.
                      # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
                      # in order to allow for the original file path to be hashed.
                      # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
                      # Default: file
                      hashing-strategy: "file"

                      # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
                      # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
                      check-sibling-md5: false
                     }
                }
            }
          }
        }
    }
}
